{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CS3610 Backpropagation Assignment by Anosan J - 200040B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights(layer_dims, path):\n",
    "    # Initialize a list to hold the reshaped weights for each layer\n",
    "    reshaped_weights = []\n",
    "\n",
    "    # Start index for extracting weights from the CSV\n",
    "    start_idx = 0\n",
    "\n",
    "    # Iterate through layers_dims to reshape and assign the weights\n",
    "    for i in range(1, len(layers_dims)):\n",
    "        # Determine the number of input and output units for this layer\n",
    "        input_units = layers_dims[i-1]\n",
    "        output_units = layers_dims[i]\n",
    "\n",
    "        # Extract the relevant rows for this layer\n",
    "        layer_weights = np.genfromtxt(path, delimiter=',', \n",
    "                                    usecols=range(1, 1 + output_units), \n",
    "                                    skip_header=start_idx, max_rows=input_units)\n",
    "        \n",
    "        # Append the reshaped weights to the list\n",
    "        reshaped_weights.append(layer_weights)\n",
    "\n",
    "        # Update the start index for the next layer\n",
    "        start_idx += input_units\n",
    "\n",
    "    # Convert the list of reshaped weights to a tuple\n",
    "    reshaped_weights = tuple(reshaped_weights)\n",
    "    return reshaped_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_biases(layer_dims, path):\n",
    "\n",
    "    # Initialize a list to hold the reshaped biases for each layer\n",
    "    reshaped_biases = []\n",
    "\n",
    "    # Start index for extracting biases from the CSV\n",
    "    start_idx = 0\n",
    "\n",
    "    # Iterate through layers_dims to reshape and assign the biases\n",
    "    for i in range(1, len(layers_dims)):\n",
    "        # Determine the number of output units for this layer\n",
    "        output_units = layers_dims[i]\n",
    "\n",
    "        # Extract the relevant columns for this layer (bias values)\n",
    "        layer_biases = np.genfromtxt(path, delimiter=',', \n",
    "                                    usecols=range(1, 1 + output_units),\n",
    "                                    skip_header=start_idx, max_rows=1)\n",
    "        \n",
    "        # Append the bias values to the list\n",
    "        reshaped_biases.append(layer_biases)\n",
    "\n",
    "        # Update the start index for the next layer\n",
    "        start_idx += 1\n",
    "\n",
    "    # Convert the list of reshaped biases to a tuple\n",
    "    reshaped_biases = tuple(reshaped_biases)\n",
    "    return reshaped_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaped_weights = load_weights(layers_dims, './Assignment_1/Task_1/a/w.csv')\n",
    "# reshaped_biases = load_biases(layers_dims, './Assignment_1/Task_1/a/b.csv')\n",
    "def print_loaded(weights, biases):\n",
    "\n",
    "    # Print the reshaped weights for each layer\n",
    "    for i, weights in enumerate(weights):\n",
    "        print(f\"Shape of Weights for Layer {i + 1}:\")\n",
    "        print(weights.shape)\n",
    "        print(\"\\n\")\n",
    "        print(f\"Weights for Layer {i + 1}:\")\n",
    "        print(weights)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    # Print the reshaped biases for each layer\n",
    "    for i, biases in enumerate(biases):\n",
    "        print(f\"Shape of Biases for Layer {i + 1}:\")\n",
    "        print(biases.shape)\n",
    "        print(\"\\n\")\n",
    "        print(f\"Biases for Layer {i + 1}:\")\n",
    "        print(biases)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the softmax function\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))  # Subtract max for numerical stability\n",
    "    return exp_x / exp_x.sum(axis=0, keepdims=True)\n",
    "\n",
    "# Define activation functions\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Pass Function\n",
    "def forward_pass(data_point, weights, biases):\n",
    "    num_layers = len(weights)\n",
    "    \n",
    "    # Initialize activations\n",
    "    activations = [data_point.astype(np.float32)]\n",
    "    \n",
    "    # Forward pass through layers\n",
    "    for layer in range(num_layers - 1):\n",
    "        z = np.dot(activations[-1], weights[layer].astype(np.float32)) + biases[layer].astype(np.float32)\n",
    "        a = relu(z)\n",
    "        activations.append(a)\n",
    "    \n",
    "    # Last layer with softmax activation\n",
    "    z_final = np.dot(activations[-1], weights[-1].astype(np.float32)) + biases[-1].astype(np.float32)\n",
    "    a_final = softmax(z_final.T)\n",
    "    activations.append(a_final)\n",
    "    \n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward Pass Function\n",
    "def backward_pass(activations, label_onehot, weights, biases):\n",
    "    num_layers = len(weights)\n",
    "    grads = []\n",
    "\n",
    "    # Calculate gradient for the last layer\n",
    "    grad_a = activations[-1].T - label_onehot\n",
    "    grad_z = grad_a  # Gradient of the loss with respect to the pre-activation of the output layer\n",
    "    grad_W = np.dot(grad_z.T, activations[-2])  # Gradient of the loss with respect to weights\n",
    "    grad_B = np.sum(grad_z.T, axis=1, keepdims=True)  # Gradient of the loss with respect to biases\n",
    "    grads.append((grad_W, grad_B))\n",
    "\n",
    "    # Backpropagate through hidden layers\n",
    "    for layer in range(num_layers - 2, -1, -1):\n",
    "        grad_a = np.dot(weights[layer + 1], grad_z.T)\n",
    "        grad_z = grad_a.T * (activations[layer + 1] > 0)\n",
    "        grad_W = np.dot(grad_z.T, activations[layer])\n",
    "        grad_B = np.sum(grad_z.T, axis=1, keepdims=True)\n",
    "        grads.append((grad_W, grad_B))\n",
    "    \n",
    "    # Reverse the list of gradients for consistency\n",
    "    grads = list(reversed(grads))\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the loss (cross-entropy)\n",
    "# loss = -np.log(a2[label[0]])  # Assuming label[0] is the correct class, and label_onehot was created correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(grads, output_gradient, output_bias):\n",
    "    # Iterate through gradients and save them to the same CSV file\n",
    "    for index, grad in enumerate(grads):\n",
    "        grad_W = grad[0]\n",
    "        grad_result = pd.DataFrame(grad_W.T)\n",
    "        if index==0:\n",
    "            grad_result.to_csv(output_gradient, index=False, header=False)\n",
    "        else:\n",
    "            grad_result.to_csv(output_gradient, mode='a', index=False, header=False)\n",
    "    \n",
    "    # Iterate through gradients and save them to the same CSV file\n",
    "    for index, grad in enumerate(grads):\n",
    "        grad_W = grad[1]\n",
    "        grad_result = pd.DataFrame(grad_W.T)\n",
    "        if index==0:\n",
    "            grad_result.to_csv(output_bias, index=False, header=False)\n",
    "        else:\n",
    "            grad_result.to_csv(output_bias, mode='a', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data\n",
    "data_point= np.array([-1, 1, 1, 1, -1, -1, 1, -1, 1, 1, -1, -1, 1, 1]).reshape(-1, 14).astype(np.float32)\n",
    "\n",
    "# One-hot encoded label\n",
    "label = np.array([3]).reshape(-1, 1)\n",
    "label_onehot = np.eye(4)[label.reshape(-1)].astype(np.float32)  # One-hot encoding of label 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the layers dimensions\n",
    "layers_dims = [14, 100, 40, 4]\n",
    "\n",
    "weights = load_weights(layers_dims, './Assignment_1/Task_1/a/w.csv')\n",
    "biases = load_biases(layers_dims, './Assignment_1/Task_1/a/b.csv')\n",
    "\n",
    "activations = forward_pass(data_point, weights, biases)\n",
    "grads = backward_pass(activations, label_onehot, weights, biases)\n",
    "# Define the output file path\n",
    "output_gradient = './Assignment_1/Answer/dw.csv'\n",
    "# Define the output file path\n",
    "output_bias = './Assignment_1/Answer/db.csv'\n",
    "\n",
    "save_file(grads, output_gradient, output_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(layer_dims, file_path):\n",
    "        \n",
    "    weights = load_weights(layers_dims, './Assignment_1/Task_1/b/w-' + file_path)\n",
    "    biases = load_biases(layers_dims, './Assignment_1/Task_1/b/b-' + file_path)\n",
    "\n",
    "    activations = forward_pass(data_point, weights, biases)\n",
    "    grads = backward_pass(activations, label_onehot, weights, biases)\n",
    "    # Define the output file path\n",
    "    output_gradient = './Assignment_1/Answer/b/dw-' + file_path\n",
    "    # Define the output file path\n",
    "    output_bias = './Assignment_1/Answer/b/db-' + file_path\n",
    "\n",
    "    save_file(grads, output_gradient, output_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the layers dimensions\n",
    "layers_dims = [14, 100, 40, 4]\n",
    "test1file = '100-40-4.csv'\n",
    "\n",
    "run(layers_dims, test1file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dims = [14, 28, 28, 28, 28, 28, 28, 4]\n",
    "test2file = '28-6-4.csv'\n",
    "\n",
    "run(layers_dims, test2file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dims = [14 for i in range(29)]\n",
    "layers_dims.append(4)\n",
    "test3file = '14-28-4.csv'\n",
    "run(layers_dims, test3file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.loadtxt('./Assignment_1/Task_2/x_train.csv', delimiter=',')\n",
    "x_test = np.loadtxt('./Assignment_1/Task_2/x_test.csv', delimiter=',')\n",
    "y_train = np.loadtxt('./Assignment_1/Task_2/y_train.csv', delimiter=',')\n",
    "y_test = np.loadtxt('./Assignment_1/Task_2/y_test.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype(int)  # Convert to integer type\n",
    "y_test = y_test.astype(int)    # Convert to integer type\n",
    "\n",
    "y_train_onehot = np.eye(4)[y_train.reshape(-1)].astype(np.float32)  # One-hot encoding of labels\n",
    "y_test_onehot = np.eye(4)[y_test.reshape(-1)].astype(np.float32)    # One-hot encoding of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Compute the cross-entropy cost.\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (number of examples, number of classes)\n",
    "    Y -- true \"label\" vector (e.g., one-hot encoded), shape (number of examples, number of classes)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[0]  # Number of examples\n",
    "    \n",
    "    # Compute cross-entropy loss\n",
    "    epsilon = 1e-7  # Small constant to avoid division by zero and logarithm of zero\n",
    "    AL = np.clip(AL, epsilon, 1 - epsilon)  # Clip values to avoid numerical instability\n",
    "    \n",
    "    # Compute the cost\n",
    "    cost = -1 / m * np.sum(Y * np.log(AL + epsilon) + (1 - Y) * np.log(1 - AL + epsilon))\n",
    "    \n",
    "    # Ensure cost is a scalar (e.g., shape ())\n",
    "    cost = np.squeeze(cost)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(predicted_labels, true_labels):\n",
    "    \"\"\"\n",
    "    Compute the accuracy of predicted labels compared to true labels.\n",
    "\n",
    "    Arguments:\n",
    "    predicted_labels -- array of predicted labels, shape (number of examples, number of classes)\n",
    "    true_labels -- array of true labels, shape (number of examples, number of classes)\n",
    "\n",
    "    Returns:\n",
    "    accuracy -- the accuracy as a percentage\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compare predicted labels to true labels and count correct predictions\n",
    "    correct_predictions = np.sum(np.argmax(predicted_labels, axis=1) == np.argmax(true_labels, axis=1))\n",
    "    \n",
    "    # Calculate accuracy as a percentage\n",
    "    accuracy = (correct_predictions / true_labels.shape[0]) * 100.0\n",
    "    \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_iterations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 54\u001b[0m\n\u001b[0;32m     51\u001b[0m weights \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(load_weights(layers_dims, \u001b[39m'\u001b[39m\u001b[39m./Assignment_1/Task_1/a/w.csv\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m     52\u001b[0m biases \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(load_biases(layers_dims, \u001b[39m'\u001b[39m\u001b[39m./Assignment_1/Task_1/a/b.csv\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m---> 54\u001b[0m costs, train_accuracies, test_accuracies \u001b[39m=\u001b[39m train_neural_network(x_train, y_train_onehot, x_test, y_test_onehot, lr, num_iterations, weights, biases)\n\u001b[0;32m     55\u001b[0m results[lr] \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mcosts\u001b[39m\u001b[39m'\u001b[39m: costs, \u001b[39m'\u001b[39m\u001b[39mtrain_accuracies\u001b[39m\u001b[39m'\u001b[39m: train_accuracies, \u001b[39m'\u001b[39m\u001b[39mtest_accuracies\u001b[39m\u001b[39m'\u001b[39m: test_accuracies}\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num_iterations' is not defined"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "def train_neural_network(x_train, y_train, x_test, y_test, learning_rate, num_iterations, weights, biases):\n",
    "\n",
    "    costs = []\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        # Forward propagation\n",
    "        activations = forward_pass(x_train, weights, biases)    \n",
    " \n",
    "        # Compute cost\n",
    "        cost = compute_cost(activations[-1], y_train.T)\n",
    "        costs.append(cost)      \n",
    "        # Backpropagation\n",
    "        grads = backward_pass(activations, y_train, weights, biases) \n",
    "        # Update weights and biases using gradient descent\n",
    "        for layer in range(len(weights)):\n",
    "            biases[layer].reshape(-1, 1)\n",
    "            weights[layer] -= (learning_rate * grads[layer][0].T)            \n",
    "            biases[layer] -= (learning_rate * grads[layer][1].T)\n",
    "        \n",
    "        # Calculate training accuracy\n",
    "        train_accuracy = compute_accuracy(activations[-1], y_train)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "        # Forward propagation on test data\n",
    "        test_activations = forward_pass(x_test, weights, biases)\n",
    "        \n",
    "        # Calculate test accuracy\n",
    "        test_accuracy = compute_accuracy(test_activations[-1], y_test)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}, Cost: {cost}, Train Accuracy: {train_accuracy}, Test Accuracy: {test_accuracy}\")\n",
    "    \n",
    "    return costs, train_accuracies, test_accuracies\n",
    "\n",
    "\n",
    "# Initialize weights and biases (you should load them as in your original code)\n",
    "# Define the layers dimensions\n",
    "layers_dims = [14, 100, 40, 4]\n",
    "\n",
    "# Define the learning rates to test\n",
    "learning_rates = [1, 0.1, 0.001]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    # Reset weights and biases here if needed\n",
    "    weights = list(load_weights(layers_dims, './Assignment_1/Task_1/a/w.csv'))\n",
    "    biases = list(load_biases(layers_dims, './Assignment_1/Task_1/a/b.csv'))\n",
    "    \n",
    "    costs, train_accuracies, test_accuracies = train_neural_network(x_train, y_train_onehot, x_test, y_test_onehot, lr, num_iterations, weights, biases)\n",
    "    results[lr] = {'costs': costs, 'train_accuracies': train_accuracies, 'test_accuracies': test_accuracies}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAj8AAAEYCAYAAACz7ZckAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAnrUlEQVR4nO3de5gcZZn38e+PyQlIMBAChCSYqBEJAgFGhOAhLIIknHR9WY4CsppFFxRc5Pi64uq7y4KrwIqyCChIEJSDsBrkJNnoCoQJhkBIkACBDEkgBnLgnIT7/aOeCZ2hZ6Znpk/T9ftcV13dVfVU1d093ffc/dTT1YoIzMzMzPJik1oHYGZmZlZNLn7MzMwsV1z8mJmZWa64+DEzM7NccfFjZmZmueLix8zMzHLFxU+OSLpD0gnlbtuXSDpQ0q9rHUepJN0i6aBax2F9U728jyWdL+m6WseRR5L+TdJptY6jFJIGSlogaZtKH8vFT52T9ErB9Lak1wvmj+3OviJickRcU+623SVpC0kXS3ouPY6FaX7rXuxzkqTWEpr+K3BBwXYh6QNF9neipPUpvtWSHpF0SE/j64ikEZJul7QkxTKmXZMLgP9X7uNa/arVe74WJB1b8NheT493w+Pvwf7GpPdRvxLanpja/l3Poq9/koYDxwP/leY7zJOSfibprfTcvyTpbkkfqkBM+0m6T9IqSYsK10XEm8DVwFnlPm57Ln7qXEQMbpuA54BDC5ZNa2tXypu9HkgaANwL7AwcBGwBTARWAHtV+NgfAd4TEQ+UuMn96XkfCvwIuEHS0DKH9TbwO+BzxVZGxCxgC0nNZT6u1alGe893JiKmFTzWycCSdo+/kk4AXkq3VVPlv9uJwPSIeL3E9hem530k8DxwVQViepWswPlGB+uvB06QNLACx97AxU8f1VbBSzpL0jLgp5K2lPQbScslvZzujyrYZoakL6b7J0r6o6TvpbbPSJrcw7ZjJc2UtEbSPZIu66SL+3hgB+CzEfF4RLwdES9GxHciYnra307p+CslzZN0WMGxpkh6PB3reUlnSNocuAPYvuBT4/ZFjj0Z+J/uPtcR8Tbwc2BzYFx3t+9i3y9ExI+AhzppNgM4uJzHtb6n0u/5Isc7W9JT6b32uKTPFqwrJSf8T9r2bqDbvbqStpd0c3psz0j6asG6vSS1KOuVfUHS99Oqmel2ZcoD+3Sw7/cCnwSmAp+WtG3BuiZJ5xY89tmSRqd1O6cekZfScc9Ny38m6bsF+9ioh0XSovR3mwu8KqlfZ89v2uZLkuYXrN9D0jck3dyu3X9KuriDp7GnOe914JfAhO5uW8K+Z0XEz4GnO1jfCrwM7F3uYxdy8dO3bQdsBbyX7E28CfDTNL8D8Drww062/yjwBFliuhC4SpJ60PZ6YBYwDDgf+Hwnx/wU8LuIKNqlLak/8N/AXcA2wKnANEk7piZXAf8QEUOADwO/j4hXefenxiVFdr9LegzdIqkJ+AKwFni2gzY7pGKto+mY7h63wHxgt15sb42jmu/5p4CPA+8Bvg1cJ2lEifu6Hpid1n2HbvauSNqELA88QtYLsT9wmqRPpyaXAJdExBbA+8n+UQN8It0OTXng/g4OcTzQEhE3k72/Ck8nfh04GphC1jN9EvCapCHAPWQ9tdsDHyDrxS7V0WQfYoZGxDo6eX4lHUGWS49PMRxG1jt+HXCQUg+0sl6kI8k+nBXT05y3eYp3YSdtjuki5+3Q3eMWqHjOc/HTt70NfCsi3oyI1yNiRUTcHBGvRcQasrEin+xk+2cj4icRsR64BhgBbNudtukF/hHgnyPirYj4I3B7J8ccBiztZP3ewGDggrS/3wO/IXsjQlaAjJe0RUS8HBEPd7Kv9oYCa7rRfm9JK4E3gO8Bx0XEi8UaRsRzETG0k+n6bhy3vTUpdrOqvecj4lcRsST1zt4IPMnGp6a7ygnfTHHOJCtkuuMjwPCI+JeUB54GfgIcldavBT4gaeuIeKUbp7LbHE9WoJFuC4uzLwL/NyKeiMwjEbECOARYFhH/ERFvRMSaiHiwG8e8NCIWt52C6uL5/SLZKaiHUgwLI+LZiFhK1rt1RGp3EPDXiJjdwTGH0r2cd0bKeWuAj9HJB9mIuL6LnPdcN47bXsVznoufvm15RLzRNiNpM0n/JelZSavJ3iRDU89FMcva7kTEa+luR+fZO2q7PfBSwTKAxZ3EvIIsSXZke2BxOtXU5lmyT3+QjY2ZAjybutWLdmt34GVgSDfaPxARQ4EtyQq6j3dj23IaAqys0bGtvlTtPS/peElz2j7Jk/W0Fp6+6iwnvJx6ZNsU7THtxHvJTmOvLDj+ubxTqP098EFggaSH1I0vI0jaFxgL3JAWXQ/sImlCmh9N1ivTXkfLS7VRXuzi+e3sWNcAx6X7x9Fxrw90P+d9L+W8MWS9iDt22rpyKp7zXPz0bdFu/p/IXqwfTd3BbV3AHXVrl8NSYCtJmxUsG91J+3vIzrFv3sH6JcDo1O3dZgeywXekT0KHk50S+zXvdHe3fy6KmUuWMLslnaL7CvB5SbsXa5NOe73SydStb+m0sxNZ979ZVd7zaUzMT4BTgGHpH+JjJe53KbBlu/d4d0+BLAaeadeTMCQipgBExJMRcTRZHvh34KZ0vFLywAlkj2OOsrFTbb03xxcc+/0dxFRsOWSDeAtz4HZF2myIrYTnt7Nj/RrYVdKHyXqjpnXQDnqe854DvgZcImnTYm208Tf1ik29Oe1V8Zzn4qexDCGr1ldK2gr4VqUPGBHPAi3A+ZIGpJ6YQzvZ5Odkb+ybJX1I0iaShikbYDiFLBG9Cpwpqb+kSWl/N6T9HyvpPRGxFlgNrE/7fQEYJuk9nRx7OsVPCQyQNKhgeten5tTtfSXwzx08D88VjDcqNnWYoCQNAtq+2TAwzRf6JNmAbrP2KvWebysklgNI+gJZz0SXCnLCt9N79mN0nhOKmQWsVjZIeFNlg5A/rOwbm0g6TtLw1EO8Mm2zPsX7NvC+YjtN762/IxsvNaFgOhU4No2huRL4jqRxyuwqaRjZ6fftJJ2m7Ho0QyR9NO16DjBF0laStgNO6+LxdfX8Xkl2CmrPFMMHUsFE6vm7iTTWsovTS0VzXrt8N0h697iviLib7MPo1GI7joJv6nUwFY0r5fxBQP9sVoOUfQu4bf1IsnFt3T2V2S0ufhrLxcCmwF/JXji/q9JxjwX2ITul9V3gRuDNYg0ju47Dp4AFwN1kBcwssu7eByPiLbLBfZPJHsePgOMjYkHaxeeBRamL/2RS929a/wvg6dSN/K5ve6XxQasKElabeWT/QNqmL3TwOC8mS3C7dvZk9MDrQNsA8AVpHtjw9fxXI/vKu1l7F1OB93xEPA78B3A/2QeLXYD/7cYujiEbEP0SWUF2bTePv56sYJoAPEP2+K4kGxwM2ViXecquBXQJcFQah/Ma2bin/015oP03hj5D9v66NiKWtU1kX6RoSvv9PlmP8l1k+ekqYNM0puqAFNcysjE6+6X9/pysp2JR2u7GLh5fp89vRPwqPY7ryca//JqsIGhzTdqms1NekD3vU9r13oxk43z3Oh33Ml1E9kG0nF87/0Q65nTeGaR/V8H6Y4Br0v+KilFEKb2EZqWTdCOwICIq3vPUXZIOBL4SEZ+pdSylUPa11qsiXQbAzCydUloAbBcRq7to+6/AixFxcTVi641UZD0CfCI6+HJJ2Y7l4sd6K/VOvET2Ce1Ask8p+0TEn2sZl5lZo0njIb8PbBERJ9U6nr6qz18h1OrCdsAtZF9jbwW+7MLHzKy80qDuF8i+Peff/OsF9/yYmZlZrnjAs5mZmeVKrk57bb311jFmzJhah2GWK7Nnz/5rRAyvdRzV5nxjVn2l5ptcFT9jxoyhpaWl1mGY5Yqk7l7dtyE435hVX6n5xqe9zMzMLFdc/JiZmVmuuPgxMzOzXMnVmB8zM7NGtXbtWlpbW3njjTdqHUrFDRo0iFGjRtG/f/8ebe/ix8zMrAG0trYyZMgQxowZQ5HfKm0YEcGKFStobW1l7NixPdqHT3uZmZk1gDfeeINhw4Y1dOEDIIlhw4b1qofLxY+ZmVmDaPTCp01vH6eLHzMzM8sVFz9mZmbWaytWrGDChAlMmDCB7bbbjpEjR26Yf+uttzrdtqWlha9+9atVitQDns3MzKwMhg0bxpw5cwA4//zzGTx4MGecccaG9evWraNfv+JlR3NzM83NzdUIE3DPj5mZmVXIiSeeyNe//nX2228/zjrrLGbNmsXEiRPZfffdmThxIk888QQAM2bM4JBDDgGywumkk05i0qRJvO997+PSSy8te1zu+TEzM2sw3/7veTy+ZHVZ9zl++y341qE7d3u7v/zlL9xzzz00NTWxevVqZs6cSb9+/bjnnns499xzufnmm9+1zYIFC7jvvvtYs2YNO+64I1/+8pd7fE2fYlz8mJmZWcUcccQRNDU1AbBq1SpOOOEEnnzySSSxdu3aotscfPDBDBw4kIEDB7LNNtvwwgsvMGrUqLLF5OLHzMyswfSkh6ZSNt988w33v/nNb7Lffvtx6623smjRIiZNmlR0m4EDB26439TUxLp168oak8f8mJmZWVWsWrWKkSNHAvCzn/2sZnG4+DEzM7OqOPPMMznnnHPYd999Wb9+fc3iUETU7ODV1tzcHC0tLbUOwyxXJM2OiOp9h7VOON9Ytc2fP5+ddtqp1mFUTbHHW2q+cc+PmZmZ5YqLHzMzM8sVFz9mZmaWKy5+zMzMLFdc/JiZmVmuuPgxMzOzXPEVns3MzKzXVqxYwf777w/AsmXLaGpqYvjw4QDMmjWLAQMGdLr9jBkzGDBgABMnTqx4rC5+zMzMrNeGDRvGnDlzgOyX2QcPHswZZ5xR8vYzZsxg8ODBVSl+anraS9JBkp6QtFDS2UXWS9Klaf1cSXu0W98k6c+SflO9qM2sr3LOMauu2bNn88lPfpI999yTT3/60yxduhSASy+9lPHjx7Prrrty1FFHsWjRIi6//HJ+8IMfMGHCBP7whz9UNK6a9fxIagIuAw4AWoGHJN0eEY8XNJsMjEvTR4Efp9s2XwPmA1tUJWgz67OccyxX7jgblj1a3n1utwtMvqDk5hHBqaeeym233cbw4cO58cYbOe+887j66qu54IILeOaZZxg4cCArV65k6NChnHzyyd3uLeqpWvb87AUsjIinI+It4Abg8HZtDgeujcwDwFBJIwAkjQIOBq6sZtBm1mc555hV0Ztvvsljjz3GAQccwIQJE/jud79La2srALvuuivHHnss1113Hf36Vb8fppZjfkYCiwvmW9n4E1ZHbUYCS4GLgTOBIZ0dRNJUYCrADjvs0KuAzaxPq3jOcb6xutGNHppKiQh23nln7r///net++1vf8vMmTO5/fbb+c53vsO8efOqGlste35UZFn7X1kt2kbSIcCLETG7q4NExBUR0RwRzW2jzs0slyqec5xvzN4xcOBAli9fvqH4Wbt2LfPmzePtt99m8eLF7Lffflx44YWsXLmSV155hSFDhrBmzZqqxFbL4qcVGF0wPwpYUmKbfYHDJC0i67r+G0nXVS5UM2sAzjlmVbTJJptw0003cdZZZ7HbbrsxYcIE/vSnP7F+/XqOO+44dtllF3bffXdOP/10hg4dyqGHHsqtt97a2AOegYeAcZLGAs8DRwHHtGtzO3CKpBvIuqdXRcRS4Jw0IWkScEZEHFeluM2sb3LOMauS888/f8P9mTNnvmv9H//4x3ct++AHP8jcuXMrGdYGNSt+ImKdpFOAO4Em4OqImCfp5LT+cmA6MAVYCLwGfKFW8ZpZ3+acY2ZtanqRw4iYTpZsCpddXnA/gH/sYh8zgBkVCM/MGoxzjpmBf9vLzMysYWT1e+Pr7eN08WNmZtYABg0axIoVKxq+AIoIVqxYwaBBg3q8D/+2l5mZWQMYNWoUra2tLF++vNahVNygQYMYNWpUj7d38WNmZtYA+vfvz9ixY2sdRp/g015mZmaWKy5+zMzMLFdc/JiZmVmuuPgxMzOzXHHxY2ZmZrni4sfMzMxyxcWPmZmZ5YqLHzMzM8sVFz9mZmaWKy5+zMzMLFdc/JiZmVmuuPgxMzOzXHHxY2ZmZrni4sfMzMxyxcWPmZmZ5YqLHzMzM8sVFz9mZmaWKy5+zMzMLFdc/JiZmVmuuPgxMzOzXHHxY2ZmZrni4sfMzMxyxcWPmZmZ5YqLHzMzM8sVFz9mZmaWKy5+zMzMLFdc/JiZmVmu1LT4kXSQpCckLZR0dpH1knRpWj9X0h5p+WhJ90maL2mepK9VP3oz62ucc8wMalj8SGoCLgMmA+OBoyWNb9dsMjAuTVOBH6fl64B/ioidgL2BfyyyrZnZBs45Ztamlj0/ewELI+LpiHgLuAE4vF2bw4FrI/MAMFTSiIhYGhEPA0TEGmA+MLKawZtZn+OcY2ZAbYufkcDigvlW3p1MumwjaQywO/BgsYNImiqpRVLL8uXLexuzmfVdFc85zjdmfUMtix8VWRbdaSNpMHAzcFpErC52kIi4IiKaI6J5+PDhPQ7WzPq8iucc5xuzvqGWxU8rMLpgfhSwpNQ2kvqTJaFpEXFLBeM0s8bgnGNmQG2Ln4eAcZLGShoAHAXc3q7N7cDx6RsYewOrImKpJAFXAfMj4vvVDdvM+ijnHDMDoF+tDhwR6ySdAtwJNAFXR8Q8SSen9ZcD04EpwELgNeALafN9gc8Dj0qak5adGxHTq/gQzKwPcc4xszaKaH/Ku3E1NzdHS0tLrcMwyxVJsyOiudZxVJvzjVn1lZpvfIVnMzMzyxUXP2ZmZpYrLn7MzMwsV1z8mJmZWa64+DEzM7NccfFjZmZmueLix8zMzHLFxY+ZmZnliosfMzMzyxUXP2ZmZpYrLn7MzMwsV1z8mJmZWa64+DEzM7NccfFjZmZmueLix8zqiqRDJDk3mVnFOMGYWb05CnhS0oWSdqp1MGbWeFz8mFldiYjjgN2Bp4CfSrpf0lRJQ2ocmpk1CBc/ZlZ3ImI1cDNwAzAC+CzwsKRTaxqYmTUEFz9mVlckHSrpVuD3QH9gr4iYDOwGnFHT4MysIfSrdQBmZu0cAfwgImYWLoyI1ySdVKOYzKyBuPgxs3rzLWBp24ykTYFtI2JRRNxbu7DMrFH4tJeZ1ZtfAW8XzK9Py8zMysLFj5nVm34R8VbbTLo/oIbxmFmDcfFjZvVmuaTD2mYkHQ78tYbxmFmD8ZgfM6s3JwPTJP0QELAYOL62IZlZI3HxY2Z1JSKeAvaWNBhQRKypdUxm1lhKKn4k/TwiPt/VMjOzcpB0MLAzMEgSABHxLzUNyswaRqljfnYunJHUBOxZ/nDMLO8kXQ4cCZxKdtrrCOC9NQ3KzBpKp8WPpHMkrQF2lbQ6TWuAF4HbqhKhmeXNxIg4Hng5Ir4N7AOMrnFMZtZAOi1+IuLfImIIcFFEbJGmIRExLCLOqVKMZpYvb6Tb1yRtD6wFxtYwHjNrMKWe9vqNpM0BJB0n6fuS3A1tZpXw35KGAhcBDwOLgF/UMiAzayylFj8/JvsUthtwJvAscG1vDy7pIElPSFoo6ewi6yXp0rR+rqQ9St3WzPoeSZsA90bEyoi4mWysz4ci4p/LtH/nHDMrufhZFxEBHA5cEhGXAEN6c+A0aPoyYDIwHjha0vh2zSYD49I0lawIK3VbM+tjIuJt4D8K5t+MiFXl2Ldzjpm1KfU6P2sknQN8Hvh4SgT9e3nsvYCFEfE0gKQbyIqrxwvaHA5cmwqvByQNlTQCGFPCtmbWN90l6XPALem9Xy51l3Me+NGXGLJyfm92YZYLa4buxN5f+UnZ9ldqz8+RwJvASRGxDBhJdj6+N0aSXbm1TWtaVkqbUrYFQNJUSS2SWpYvX97LkM2sCr5O9kOmb7Z9w1TS6jLst+I5x/nGrG8oqecnIpZJmgZ8RNIhwKyI6O2YHxU7VIltStk2WxhxBXAFQHNzczk/RZpZBaRvmFZCxXNOd/NNOT/JmlnpSr3C89+R9fTMIEsC/ynpGxFxUy+O3crG1+4YBSwpsc2AErY1sz5I0ieKLY+Imb3ctXOOmQGlj/k5D/hIRLwIIGk4cA/Qm+LnIWCcpLHA88BRwDHt2twOnJLOr38UWBURSyUtL2FbM+ubvlFwfxDZWJ3ZwN/0cr/OOWYGlF78bNJW+CQrKH28UFERsU7SKcCdQBNwdUTMk3RyWn85MB2YAiwEXgO+0Nm2vYnHzOpDRBxaOC9pNHBhGfbrnGNmQPaLyV03ki4CduWdC40dCcyNiLMqGFvZNTc3R0tLS63DMMsVSbMjorkX24ss3+xSxrAqzvnGrPpKzTed9vxI+gCwbUR8Q9LfAh8jG/NzPzCtLJGamRWQ9J+8M5h4E2AC8EjNAjKzhtPVaa+LgXMBIuIW4BYASc1p3aEdbWhm1kOF3SXrgF9ExP/WKhgzazxdFT9jImJu+4UR0SJpTGVCMrOcuwl4IyLWQ3Z1ZUmbRcRrNY7LzBpEV4OWB3WybtNyBmJmltzLxvllU7Jvl5qZlUVXxc9Dkr7UfqGkvyf76qmZWbkNiohX2mbS/c1qGI+ZNZiuTnudBtwq6VjeKXaayS749dkKxmVm+fWqpD0i4mEASXsCr9c4JjNrIJ0WPxHxAjBR0n7Ah9Pi30bE7ysemZnl1WnAryS1XUF5BNnlNczMyqLU3/a6D7ivwrGYmRERD0n6ELAj2aU1FkTE2hqHZWYNpFdXaTYzKzdJ/whsHhGPRcSjwGBJX6l1XGbWOFz8mFm9+VJErGybiYiXgXd98cLMrKdc/JhZvdkk/aQFkF3nh+xLFmZmZVHqD5uamVXLncAvJV1O9jMXJwN31DYkM2skLn7MrN6cBUwFvkw24PnPZN/4MjMrC5/2MrO6EhFvAw8AT5NdV2x/YH5NgzKzhuKeHzOrC5I+CBwFHA2sAG4EiIj9ahmXmTUeFz9mVi8WAH8ADo2IhQCSTq9tSGbWiHzay8zqxeeAZcB9kn4iaX+yMT9mZmXl4sfM6kJE3BoRRwIfAmYApwPbSvqxpANrGpyZNRQXP2ZWVyLi1YiYFhGHAKOAOcDZtY3KzBqJix8zq1sR8VJE/FdE/E2tYzGzxuHix8zMzHLFxY+ZmZnliosfMzMzyxUXP2ZmZpYrLn7MzMwsV1z8mJmZWa64+DEzM7NccfFjZmZmueLix8zMzHLFxY+ZmZnliosfMzMzy5WaFD+StpJ0t6Qn0+2WHbQ7SNITkhZKOrtg+UWSFkiaK+lWSUOrFryZ9TnOOWZWqFY9P2cD90bEOOBeivxis6Qm4DJgMjAeOFrS+LT6buDDEbEr8BfgnKpEbWZ9lXOOmW1Qq+LncOCadP8a4DNF2uwFLIyIpyPiLeCGtB0RcVdErEvtHgBGVTZcM+vjnHPMbINaFT/bRsRSgHS7TZE2I4HFBfOtaVl7JwF3dHQgSVMltUhqWb58eS9CNrM+rCo5x/nGrG/oV6kdS7oH2K7IqvNK3UWRZdHuGOcB64BpHe0kIq4ArgBobm6OjtqZWd9WDznH+casb6hY8RMRn+ponaQXJI2IiKWSRgAvFmnWCowumB8FLCnYxwnAIcD+EeEkY5ZzzjlmVqpanfa6HTgh3T8BuK1Im4eAcZLGShoAHJW2Q9JBwFnAYRHxWhXiNbO+zTnHzDaoVfFzAXCApCeBA9I8kraXNB0gDS48BbgTmA/8MiLmpe1/CAwB7pY0R9Ll1X4AZtanOOeY2QYVO+3VmYhYAexfZPkSYErB/HRgepF2H6hogGbWUJxzzKyQr/BsZmZmueLix8zMzHLFxY+ZmZnliosfMzMzyxUXP2ZmZpYrLn7MzMwsV1z8mJmZWa64+DEzM7NccfFjZmZmueLix8zMzHLFxY+ZmZnliosfMzMzyxUXP2ZmZpYrLn7MzMwsV1z8mJmZWa64+DEzM7NccfFjZmZmueLix8zMzHLFxY+ZmZnliosfMzMzyxUXP2ZmZpYrLn7MzMwsV1z8mJmZWa64+DEzM7NccfFjZmZmueLix8zMzHLFxY+ZmZnliosfMzMzyxUXP2ZmZpYrLn7MzMwsV1z8mJmZWa7UpPiRtJWkuyU9mW637KDdQZKekLRQ0tlF1p8hKSRtXfmozayvcs4xs0K16vk5G7g3IsYB96b5jUhqAi4DJgPjgaMljS9YPxo4AHiuKhGbWV/mnGNmG9Sq+DkcuCbdvwb4TJE2ewELI+LpiHgLuCFt1+YHwJlAVDBOM2sMzjlmtkGtip9tI2IpQLrdpkibkcDigvnWtAxJhwHPR8QjXR1I0lRJLZJali9f3vvIzawvqkrOcb4x6xv6VWrHku4Btiuy6rxSd1FkWUjaLO3jwFJ2EhFXAFcANDc3+xObWYOqh5zjfGPWN1Ss+ImIT3W0TtILkkZExFJJI4AXizRrBUYXzI8ClgDvB8YCj0hqW/6wpL0iYlnZHoCZ9SnOOWZWqlqd9rodOCHdPwG4rUibh4BxksZKGgAcBdweEY9GxDYRMSYixpAlrD2chMysE845ZrZBrYqfC4ADJD1J9u2JCwAkbS9pOkBErANOAe4E5gO/jIh5NYrXzPo25xwz26Bip706ExErgP2LLF8CTCmYnw5M72JfY8odn5k1FuccMyvkKzybmZlZrrj4MTMzs1xx8WNmZma54uLHzMzMcsXFj5mZmeWKix8zMzPLFRc/ZmZmlisufszMzCxXXPyYmZlZrrj4MTMzs1xx8WNmZma54uLHzMzMcsXFj5mZmeWKix8zMzPLFRc/ZmZmlisufszMzCxXXPyYmZlZrrj4MTMzs1xx8WNmZma54uLHzMzMcsXFj5mZmeWKix8zMzPLFRc/ZmZmlisufszMzCxXFBG1jqFqJC0Hnq3BobcG/lqD43ZHvcfo+HqvVjG+NyKG1+C4NdWNfOPXTu/Ve3xQ/zE2Snwl5ZtcFT+1IqklIpprHUdn6j1Gx9d7fSHGPOoLf5d6j7He44P6jzFv8fm0l5mZmeWKix8zMzPLFRc/1XFFrQMoQb3H6Ph6ry/EmEd94e9S7zHWe3xQ/zHmKj6P+TEzM7Nccc+PmZmZ5YqLHzMzM8sVFz9lImkrSXdLejLdbtlBu4MkPSFpoaSzi6w/Q1JI2rqe4pN0kaQFkuZKulXS0DLF1dXzIUmXpvVzJe1R6rbl0tMYJY2WdJ+k+ZLmSfpaPcVXsL5J0p8l/aYS8VnHqvUa7iKGoq/TznKGpHNSzE9I+nSV4tzodVqH8Q2VdFPKk/Ml7VNPMUo6Pf19H5P0C0mDah2fpKslvSjpsYJl3Y5J0p6SHk3rLpWkLg8eEZ7KMAEXAmen+2cD/16kTRPwFPA+YADwCDC+YP1o4E6yC6NtXU/xAQcC/dL9fy+2fQ9i6vT5SG2mAHcAAvYGHix12zI9b72JcQSwR7o/BPhLuWPsTXwF678OXA/8phrvFU+l/+2qFEfR12lHOSOtewQYCIxNj6GpCnFu9Dqtw/iuAb6Y7g8AhtZLjMBI4Blg0zT/S+DEWscHfALYA3isYFm3YwJmAfukHHcHMLmrY7vnp3wOJ3vxk24/U6TNXsDCiHg6It4CbkjbtfkBcCZQiVHovYovIu6KiHWp3QPAqDLE1NXz0Rb3tZF5ABgqaUSJ25ZDj2OMiKUR8TBARKwB5pMlobqID0DSKOBg4Moyx2Vdq9ZruFOdvE47yhmHAzdExJsR8QywkOyxVEwHr9N6im8Lsn/kVwFExFsRsbKeYgT6AZtK6gdsBiypdXwRMRN4qd3ibsWUctkWEXF/ZJXQtRT//7YRFz/ls21ELIUsmQDbFGkzElhcMN+aliHpMOD5iHikHuNr5ySy6rq3SjleR21KjbWWMW4gaQywO/BgncV3MVnB/XaZ47KuVes1XLJ2r9OOckYt4r6Yd79O6ym+9wHLgZ+mU3NXStq8XmKMiOeB7wHPAUuBVRFxV73E1053YxqZ7rdf3ql+ZQk1JyTdA2xXZNV5pe6iyLKQtFnax4E9jQ0qF1+7Y5wHrAOmdS+6nh2vkzalbFsOvYkxWykNBm4GTouI1WWMrctjd9ZG0iHAixExW9KkMsdlXavWa7gk7V+nnQybqGrcPXid1uJ57Ud2+ubUiHhQ0iVkp2w6Uu3ncEuynpOxwErgV5KO62yTIstqfV2csv4vcPHTDRHxqY7WSXqh7VRH6oZ7sUizVrJxPW1GkXU9vp/sRflISjijgIcl7RURy+ogvrZ9nAAcAuyfuhd7q9PjddFmQAnblkNvYkRSf7J/KNMi4pY6i+//AIdJmgIMAraQdF1EdJYUrXxK+dtVRQev045yRrXj3pcir9M6iq/tmK0R0dazexNZ8VMvMX4KeCYilgNIugWYWEfxFepuTK1sPAyjtFjLPYAprxNwERsP0rqwSJt+wNNkhU7bAMedi7RbRPkHPPcqPuAg4HFgeBlj6vL5IDvPXzhYd1Z3nssaxyiy888XV/B11+P42rWZhAc8V3Wq1mu4hDiKvk47yhnAzmw88PRpqjCgOB17w+u03uID/gDsmO6fn+KrixiBjwLzyMb6iGwszan1EB8who0HPHc7JuChlNvaBjxP6fK41XjB5mEChgH3Ak+m263S8u2B6QXtppB9m+Ip4LwO9rWI8hc/vYqPbHDZYmBOmi4vU1zvOh5wMnByui/gsrT+UaC5O89lLWMEPkbW/Tq34Hnr8k1ZzeewYB+TcPFT9alar+EuYij6Ou0oZ6RtzksxP0EJ36wpY6wbXqf1Fh8wAWhJz+OvgS3rKUbg28AC4DHg52RFRE3jA35BNgZpLVkPzt/3JCagOT2up4Afkn69orPJP29hZmZmueJve5mZmVmuuPgxMzOzXHHxY2ZmZrni4sfMzMxyxcWPmZmZ5YqLHysbSa+k2zGSjinzvs9tN/+ncu7fzPoe5xzrKRc/VgljgG4lIklNXTTZKBFFxMRuxmRmjWsMzjnWDS5+rBIuAD4uaY6k0yU1SbpI0kOS5kr6BwBJkyTdJ+l6sovvIenXkmZLmidpalp2AdmvEc+RNC0ta/vEp7TvxyQ9KunIgn3PkHSTpAWSpin9doikCyQ9nmL5XtWfHTMrN+cc655qXZnTU+NPwCvpdhIFVwsGpgL/N90fSHYV1LGp3avA2IK2bVee3pTsip3DCvdd5FifA+4GmoBtyX61eETa9yqy33nZBLif7Eq2W5FdHbTtAp9Da/28efLkqWeTc46nnk7u+bFqOBA4XtIc4EGyy5ePS+tmRcQzBW2/KukR4AGyH7EbR+c+BvwiItZHxAvA/wAfKdh3a0S8TXbJ/jHAauAN4EpJfwu81svHZmb1xznHOuXix6pBwKkRMSFNYyPirrTu1Q2NpElkvz68T0TsBvyZ7Fecu9p3R94suL8e6BcR64C9yH7B+jPA77rxOMysb3DOsU65+LFKWAMMKZi/E/iypP4Akj4oafMi270HeDkiXpP0IbJf6W2ztm37dmYCR6Zz/MOBTwCzOgpM0mDgPRExHTiN7McIzaxvc86xbulX6wCsIc0F1qWu5J8Bl5B1/z6cBgAuJ/sE1N7vgJMlzSU7R/5AwborgLmSHo6IYwuW3wrsAzxC9svUZ0bEspTIihkC3CZpENknuNN79AjNrJ4451i3+FfdzczMLFd82svMzMxyxcWPmZmZ5YqLHzMzM8sVFz9mZmaWKy5+zMzMLFdc/JiZmVmuuPgxMzOzXPn/Wf79lY/m5y8AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Plot the results\n",
    "for lr, result in results.items():\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(result['costs'])\n",
    "    plt.title(f'Training Cost (LR = {lr})')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Cost')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(result['train_accuracies'], label='Train')\n",
    "    plt.plot(result['test_accuracies'], label='Test')\n",
    "    plt.title(f'Train and Test Accuracy (LR = {lr})')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3-6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
